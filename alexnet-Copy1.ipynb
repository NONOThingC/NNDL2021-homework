{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NF2IR5u8-0xo"
   },
   "source": [
    "# 3 - AlexNet\n",
    "\n",
    "In this notebook we will be implementing a modified version of [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), a neural network model that uses convolutional neural network (CNN) layers and was designed for the [ImageNet challenge](http://www.image-net.org/challenges/LSVRC/). AlexNet is famous for winning the ImageNet challenge in 2012 by beating the second place competitor by over 10% accuracy and kickstarting the interest in deep learning for computer vision.\n",
    "\n",
    "The image below shows the architecture of AlexNet.\n",
    "\n",
    "![](assets/alexnet.png)\n",
    "\n",
    "Confusingly, there are two \"paths\" of processing through the network. This is due to the original AlexNet model being implemented on two GPUs in parallel. Almost all implementations of AlexNet are now on a single GPU and our implementation is too.\n",
    "\n",
    "We are now moving on from the MNIST dataset and from now on we will be using the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. CIFAR10 consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. The classes are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. \n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-image-classification/blob/master/assets/cifar10.png?raw=1)\n",
    "\n",
    "We will also show how to intialize the weights of our neural network and how to find a suitable learning rate using a modified version of the [learning rate finder](https://arxiv.org/abs/1506.01186).\n",
    "\n",
    "Like the previous notebooks we'll implement our model, measure its performance on the dataset, and then have a short look into seeing what the model has learned.\n",
    "\n",
    "Most of this notebook will be similar to the previous ones thus we will skim over code that has been shown before. We can look at the previous notebook for a refresher if needed.\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "As always, we'll import the modules we need. A new import is the `_LRScheduler` which we will use to implement our learning rate finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYaxtUEgIafG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "# import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_mlp.ipynb\t       4_vgg.ipynb     assets\t       tut3-model.pt\r\n",
      "2_lenet.ipynb\t       5_resnet.ipynb  init_params.pt\r\n",
      "3_alexnet-Copy1.ipynb  LICENSE\t       misc\r\n",
      "3_alexnet.ipynb        README.md       tensor_log\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FcvzjWq-0xs"
   },
   "source": [
    "We set the random seed so all of our experiments can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfZxvR09IafJ"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uecfsf1f-0xw"
   },
   "source": [
    "We calculate the mean and standard deviation of our data so we can normalize it.\n",
    "\n",
    "Our dataset is made up of color images but three color channels (red, green and blue), compared to MNIST's black and white images with a single color channel. To normalize our data we need to calculate the means and standard deviations for each of the color channels independently. \n",
    "\n",
    "To do this we pass a tuple containing the axes we want to take the means and standard deviations over to the `mean` and `std` functions and we receive a list of means and standard deviations for each of the three color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "IprUgI0fIafN",
    "outputId": "9be94ef6-0588-41e8-e980-bc42c1700242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Calculated means: [0.49139968 0.48215841 0.44653091]\n",
      "Calculated stds: [0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "ROOT = '.data'\n",
    "\n",
    "train_data = datasets.CIFAR10(root = ROOT, \n",
    "                              train = True, \n",
    "                              download = True)\n",
    "\n",
    "means = train_data.data.mean(axis = (0,1,2)) / 255\n",
    "stds = train_data.data.std(axis = (0,1,2)) / 255\n",
    "\n",
    "print(f'Calculated means: {means}')\n",
    "print(f'Calculated stds: {stds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajVaWn_7-0x0"
   },
   "source": [
    "Next up is defining the transforms for data augmentation. \n",
    "\n",
    "The images in the CIFAR10 dataset are significantly more complex than the MNIST dataset. They are larger, have three times the amount of pixels and are more cluttered. This makes them harder to learn and consequently means we should use less augmentation.\n",
    "\n",
    "A new transform we use is `RandomHorizontalFlip`. This, with a probability of `0.5` as specified, flips the image horizontally. So an image of a horse facing to the right will be flipped so it will face to the left. We couldn't do this in the MNIST dataset as we are not expecting our test set to contain any flipped digits, however natural images, such as those in the CIFAR10 dataset, can potentially be flipped as they still make visual sense.\n",
    "\n",
    "As our `means` and `stds` are now already in lists we do not need to enclose them in lists as we did for the single channel images in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfzKdrpfIafR"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                           transforms.RandomRotation(5),\n",
    "                           transforms.RandomHorizontalFlip(0.5),\n",
    "                           transforms.RandomCrop(32, padding = 2),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = means, \n",
    "                                                std = stds)\n",
    "                       ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = means, \n",
    "                                                std = stds)\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPLqEffl-0x3"
   },
   "source": [
    "Next, as standard, we'll load the dataset with our transforms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "vqxev2wBIafU",
    "outputId": "0268171f-6a47-424b-a989-3b6ea275409e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR10(ROOT, \n",
    "                              train = True, \n",
    "                              download = True, \n",
    "                              transform = train_transforms)\n",
    "\n",
    "\n",
    "test_data = datasets.CIFAR10(ROOT, \n",
    "                             train = False, \n",
    "                             download = True, \n",
    "                             transform = test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjI3JE5N-0x6"
   },
   "source": [
    "...create a validation set from our training set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NyMEMaDUIafX"
   },
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data, \n",
    "                                           [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IUMURwF-0x-"
   },
   "source": [
    "...and ensure our validation set uses the test transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKvCJ2_EQUgD"
   },
   "outputs": [],
   "source": [
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transform = test_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjflicvF-0yB"
   },
   "source": [
    "We print out the number of examples in each set of data to ensure everything has gone OK so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "enX-chu-Iafc",
    "outputId": "b62f02b5-7ae1-4ff7-8ab3-5d006c811576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 45000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQq6BaNr-0yE"
   },
   "source": [
    "Now, we'll create a function to plot some of the images in our dataset to see what they actually look like.\n",
    "\n",
    "Note that by default PyTorch handles images that are arranged `[channel, height, width]`, but `matplotlib` expects images to be `[height, width, channel]`, hence we need to `permute` our images before plotting them.\n",
    "\n",
    "Ignore the `normalize` argument for now, we'll explain it shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXe4GSO7-0yH"
   },
   "source": [
    "Then, we'll actually plot the images.\n",
    "\n",
    "We get both the images and the labels from the training set and convert the labels, which are originally stored as integers, into their human readable class by using the data's `classes` dictionary.\n",
    "\n",
    "When we plot them we see lots of warnings. This is because `matplotlib` is expecting the values of every pixel to be between $[0, 1]$, however our normalization will cause them to be outside this range. By default `matplotlib` will then clip these values into the $[0,1]$ range. This clipping causes all of the images to look a bit weird - all of the colors are oversaturated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoCEaY9C-0yK"
   },
   "source": [
    "A solution to this is to *renormalize* the images so each pixel is between $[0,1]$. This is done by clipping the pixel values between the maximum and minimum within an image and then scaling each pixel between $[0,1]$ using these maximum and minimums. \n",
    "\n",
    "As we can see the images below look a lot more like we were expecting, along with the rotations and cropping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAFVQhu4-0yN"
   },
   "source": [
    "We'll be normalizing our images by default from now on, so we'll write a function that does it for us which we can use whenever we need to renormalize an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVDM7PTCswBx"
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMrfzrD0-0yQ"
   },
   "source": [
    "As before, we'll check what images look like with Sobel filters applied to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5mBc7Zy-0yS"
   },
   "source": [
    "The filters are still 2-dimensional but they are expanded to a depth of three dimensions inside the `plot_filter` function.\n",
    "\n",
    "Below is a filter which detects horizontal lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYT28zhp-0yW"
   },
   "source": [
    "Here's a filter that detects vertical lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6SjLzwv-0yY"
   },
   "source": [
    "We'll also do the same for subsampling/pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3bSOuZcS-0yb"
   },
   "source": [
    "As before, the higher filter sizes in the pooling layers means more information is lost, i.e. the image becomes lower resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BL_2X7ZF-0yl"
   },
   "source": [
    "The final bit of the data processing is creating the iterators.\n",
    "\n",
    "We use a much larger batch size here than in previous models. Generally, when using a GPU, a larger batch size means our model trains faster. Our model has significantly more parameters and the images it is training on are much larger, than the previous notebook, so will generally take longer. We offset this as much as we can by using a batch size of 256 instead of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3rhhD0HIaff"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(valid_data, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "test_iterator = data.DataLoader(test_data, \n",
    "                                batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98q8mpRN-0yp"
   },
   "source": [
    "### Defining the Model\n",
    "\n",
    "Next up is defining the model.\n",
    "\n",
    "The actual model itself is no more difficult to understand than the previous model, LeNet. It is made up of convolutional layers, pooling layers and ReLU activation functions. See the previous notebook for a refresher on these concepts. \n",
    "\n",
    "There are only two new concepts introduced here, `nn.Sequential` and `nn.Dropout`.\n",
    "\n",
    "We can think of `Sequential` as like our transforms introduced earlier for data augmentation. We provide `Sequential` with multiple layers and when the `Sequential` module is called it will apply each layer, in order, to the input. There is no difference between using a `Sequential` and having each module defined in the `__init__` and then called in `forward` - however it makes the code significantly shorter.\n",
    "\n",
    "We have one `Sequential` model, `features`, for all of the convolutional and pooling layers, then we flatten then data and pass it to the `classifier`, another `Sequential` model which is made up of linear layers and the second new concept, *dropout*.\n",
    "\n",
    "Dropout is a form of [*regularization*](https://en.wikipedia.org/wiki/Regularization_(mathematics)). As our models get larger, to perform more accurately on richer datasets, they start having a significantly higher number of parameters. The problem with lots of parameters is that our models begin to *overfit*. That is, they do not learn general image features whilst learning to classify images but instead simply memorize images within the training set. This is bad as it will cause our model to achieve poor performance on the validation/testing set. To solve this overfitting problem, we use regularization. Dropout is just one method of regularization, other common ones are *L1 regularization*, *L2 regularization* and *weight decay*.\n",
    "\n",
    "Dropout works by randomly setting a certain fraction, 0.5 here, of the neurons in a layer to zero. This effectively adds noise to the training of the neural network and causes neurons to learn with \"less\" data as they are only getting half of the information from a previous layer with dropout applied. It can also be thought of as causing your model to learn multiple smaller models with less parameters. \n",
    "\n",
    "Dropout is only applied when the model is training. It needs to be \"turned off\" when validating, testing or using the model for inference.\n",
    "\n",
    "As mentioned in the previous notebook, during the convolutional and pooling layers the activation function should be placed **after** the pooling layer to reduce computational cost.\n",
    "\n",
    "In the linear layers, dropout should be applied **after** the activation function. Although when using ReLU activation functions the same result is achieved if dropout is before or after, see [here](https://sebastianraschka.com/faq/docs/dropout-activation.html).\n",
    "\n",
    "One last thing to mention is that the very first convolutional layer has an `in_channel` of three. That is because we are handling color images that have three channels (red, green and blue) instead of the single channel grayscale images from the MNIST dataset. This doesn't change the way any of the convolutional filter works, it just means the first filter has a depth of three instead of a depth of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQq-LE22Iafk"
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 2, 1), #in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.MaxPool2d(2), #kernel_size\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 192, 3, padding = 1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(192, 384, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(384, 256, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, padding = 1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4096, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(h)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBMS4TH4XVow"
   },
   "source": [
    "We'll create an instance of our model with the desired amount of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-sDcYnBIafp"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 10\n",
    "\n",
    "model = AlexNet(OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9Dkc_uBXVoy"
   },
   "source": [
    "Then we'll see how many trainable parameters our model has. \n",
    "\n",
    "Our LeNet architecture had ~44k, but here we have 23.2M parameters - and AlexNet is a relatively small model for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "3g-pV7t2Iafs",
    "outputId": "32946ab7-a380-44fc-ea2b-33ad4ab02aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 22,682,186 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n99m5MPZXVo1"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "Next up, we'll initialize the parameters of our model.\n",
    "\n",
    "PyTorch's default initialization is usually fine however by manually trying different initialization schemes we can usually squeeze out a slight performance improvement.\n",
    "\n",
    "We initialize parameters in PyTorch by creating a function that takes in a PyTorch module, checking what type of module it is, and then using the `nn.init` methods to actually initialize the parameters.\n",
    "\n",
    "For our convolutional layers, we'll initialize the weights from a Normal distribution with a standard deviation given by:\n",
    "\n",
    "$$\\frac{\\text{gain}}{\\sqrt{\\text{fan mode}}}$$\n",
    "\n",
    "The value of $\\text{gain}$ depends on the non-linearity we will be using after the convolutional layer and we simply tell the initialization function that we are using ReLU which sets the gain to $\\sqrt{2}$. The fan mode can be either `fan_in` or `fan_out`. `fan_in` is the number of connections coming into the layer and `fan_out` is the number of connections going out of the layer. For the first convolutional layer the input is from 3x3x3 filter, so the `fan_in` is 27 and the output is 64x3x3,  so the `fan_out` is 576. We leave it to the default `fan_in` mode. This initialization scheme is called *Kaiming Normal*, also known as *He Normal*. See the [paper](https://arxiv.org/abs/1502.01852) to learn more about how it was devised.\n",
    "\n",
    "For the linear layers we initialize with a Normal distribution but this time the standard deviation is given by:\n",
    "\n",
    "$$\\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan_in} + \\text{fan_out}}}$$\n",
    "\n",
    "Confusingly, instead of just telling the initialization function which non-linearity we want to use and have it calculate the gain for us, we have to tell it what gain we want to use. Luckily, `nn.init` has a `calculate_gain` function which does that for us, and we just tell it we are using ReLUs. This type of initialize scheme is called *Xavier Normal*, also known as *Glorot Normal*. See the [paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) for the theory behind it. \n",
    "\n",
    "For both types of layer we initialize the bias terms to zeros.\n",
    "\n",
    "Why do we even need to initialize our parameters this way? See [this](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79) article for a great explanation, but the gist of it is that just like how we normalized our input data to have a mean of 0 and a standard deviation of 1, we also want the outputs of each activation function (and therefore the inputs to the subsequent layer) to also have a mean of 0 and a standard deviation of 1. These initialization schemes, by taking account the number of connections in to and out of a layer as well as the non-linearity used, help achieve this normalization effect when initializing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4NbFid9Iafw"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SkSdnytXVo3"
   },
   "source": [
    "We apply the initialization by using the model's `apply` method. This will call the given function on every module and sub-module within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "Bve8IsHsIafz",
    "outputId": "cfaf7eb5-37ab-4aad-d2de-9d3040c13317"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (2): Sigmoid()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): Sigmoid()\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-gyHsCBNXVo5"
   },
   "source": [
    "Next up is the learning rate finder. The code here is taken from a stripped down and slightly modified version of the excellent [pytorch-lr-finder](https://github.com/davidtvs/pytorch-lr-finder). \n",
    "\n",
    "**Note**: the learning rate finder is more of an art than a science. It is not going to find an exact learning rate to 10 decimal places which will always give us 100% accuracy - but it is usually going to be better than just picking a learning rate out of thin air.  \n",
    "\n",
    "The most commonly used optimizer used is Adam. Adam's default learning rate is usually a fine choice but, much like how we manually initialized our parameters to potentially get some performance improvement, we can try and calculate an optimal learning rate manually.\n",
    "\n",
    "How does the learning rate finder work? We give the finder our model, optimizer and criterion we want to use. However we give it an optimizer with a much lower learning rate than we are expecting to use. We then train the model on the batches of data from the training set - calculating the loss and updating the parameters. After each batch we increase the learning rate exponentially from the initial, extremely low learning rate to a learning rate we know will be too high. This repeats until our loss diverges (over 5x the best loss achieved) or we reach our defined maximum learning rate. \n",
    "\n",
    "At each batch we are recording the learning rate and the loss achieved on that batch. By plotting them against each other we can find a suitable learning rate - but more on how to do that in a bit.\n",
    "\n",
    "The losses calculated are usually quite noisy so we actually save the exponentially weighted average of the loss calculated. \n",
    "\n",
    "We also want to use our initialized parameters, not the ones obtained by upgrading the parameters when performing the learning rate finder. Hence we save the model parameters to disk when initializing the finder and then they are reset to our desired initialized ones just before the `range_test` function returns by loading the initial values from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-5vb3-dXVo8"
   },
   "source": [
    "To prepare to use the range finder we define an initial, very low starting learning rate and then create an instance of the optimizer we want to use with that learning rate.\n",
    "\n",
    "We then define the loss function we want to use, the device we'll use and place our model and criterion on to our device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5583ybdqIaf4"
   },
   "outputs": [],
   "source": [
    "START_LR = 1e-7\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = START_LR)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f62J8sOuXVo-"
   },
   "source": [
    "Next, we'll finally use the range finder.\n",
    "\n",
    "We first create an instance of the finder class with the model, optimizer, loss function and device. Then we use `range_test` with the training iterator, the maximum learning rate and the number of iterations we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "BcA3kgLHIaf6",
    "outputId": "e1813db9-4c1d-4c68-ad68-069bd423006b"
   },
   "outputs": [],
   "source": [
    "END_LR = 10\n",
    "NUM_ITER = 100\n",
    "\n",
    "# lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "# lrs, losses = lr_finder.range_test(train_iterator, END_LR, NUM_ITER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPd2y04tXVpE"
   },
   "source": [
    "Next, we can plot the learning rate against the loss. \n",
    "\n",
    "As our learning rate was scaled up exponentially we want to plot it on a logarithmic scale. We also do not want to plot the last few values as they are usually where the loss is very high and makes it difficult to examine the graph in detail. You can also skip the first few values as nothing interesting happens there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3zLyhwJXVpH"
   },
   "source": [
    "As we can see, the loss begins flat and then begins to decrease rapidly before reaching a minimum and starting to increase. \n",
    "\n",
    "How can we read this plot and get the optimal learning rate? According to [this](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html) article, we should look for the loss begins to flatten, this is around $10^{-2}$ below, and then reduce that by a factor of 10, which gives us a found learning rate of $10^{-3}$ or $0.001$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WImHQVcvXVpK"
   },
   "source": [
    "We can now create a new optimizer with our found learning rate.\n",
    "\n",
    "Ironically, the learning rate value we found, $0.001$ is actually Adam's default learning rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6hTusQNIagD"
   },
   "outputs": [],
   "source": [
    "FOUND_LR = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = FOUND_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HoBM4KEXVpS"
   },
   "source": [
    "The rest of the notebook is pretty similar to the previous notebooks from these tutorials.\n",
    "\n",
    "We define a function to calculate accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJj9fQ5WIagI"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGjesEGDXVpV"
   },
   "source": [
    "...and a function to implement our training loop.\n",
    "\n",
    "As we are using dropout we need to make sure to \"turn it on\" when training by using `model.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e65mN1cGIagK"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device, epoch):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    count=0\n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred, _ = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        count+=1\n",
    "#         writer.add_scalar('Batch/Train/loss', loss, epoch*len(iterator) + count)\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U0usLljsXVpY"
   },
   "source": [
    "We also define an evaluation loop, making sure to \"turn off\" dropout with `model.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnUhGD9MIagM"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkkzHBcoXVpa"
   },
   "source": [
    "Next, we define a function to tell us how long an epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RX-Mwze-IagO"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53srBoLAXVpe"
   },
   "source": [
    "Then, finally, we train our model.\n",
    "\n",
    "We get a best validation loss of ~76% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mN02GOocIagR",
    "outputId": "04ceaa5e-4894-4629-9664-422276b62bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 3.087 | Train Acc: 9.99%\n",
      "\t Val. Loss: 2.336 |  Val. Acc: 9.29%\n",
      "Epoch: 02 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 2.340 | Train Acc: 9.87%\n",
      "\t Val. Loss: 2.327 |  Val. Acc: 9.64%\n",
      "Epoch: 03 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 2.330 | Train Acc: 9.87%\n",
      "\t Val. Loss: 2.332 |  Val. Acc: 10.91%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 2.098 | Train Acc: 19.63%\n",
      "\t Val. Loss: 1.814 |  Val. Acc: 30.61%\n",
      "Epoch: 05 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 1.676 | Train Acc: 35.37%\n",
      "\t Val. Loss: 1.527 |  Val. Acc: 41.88%\n",
      "Epoch: 06 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 1.448 | Train Acc: 45.71%\n",
      "\t Val. Loss: 1.342 |  Val. Acc: 51.04%\n",
      "Epoch: 07 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 1.277 | Train Acc: 53.31%\n",
      "\t Val. Loss: 1.227 |  Val. Acc: 55.98%\n",
      "Epoch: 08 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 1.175 | Train Acc: 58.03%\n",
      "\t Val. Loss: 1.144 |  Val. Acc: 59.82%\n",
      "Epoch: 09 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 1.089 | Train Acc: 61.07%\n",
      "\t Val. Loss: 1.012 |  Val. Acc: 63.49%\n",
      "Epoch: 10 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 1.025 | Train Acc: 63.53%\n",
      "\t Val. Loss: 0.974 |  Val. Acc: 66.48%\n",
      "Epoch: 11 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.958 | Train Acc: 66.21%\n",
      "\t Val. Loss: 0.933 |  Val. Acc: 67.88%\n",
      "Epoch: 12 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.912 | Train Acc: 67.91%\n",
      "\t Val. Loss: 0.874 |  Val. Acc: 69.24%\n",
      "Epoch: 13 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.876 | Train Acc: 69.37%\n",
      "\t Val. Loss: 0.879 |  Val. Acc: 70.26%\n",
      "Epoch: 14 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.837 | Train Acc: 70.74%\n",
      "\t Val. Loss: 0.851 |  Val. Acc: 70.39%\n",
      "Epoch: 15 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.801 | Train Acc: 72.02%\n",
      "\t Val. Loss: 0.803 |  Val. Acc: 73.31%\n",
      "Epoch: 16 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.770 | Train Acc: 73.06%\n",
      "\t Val. Loss: 0.787 |  Val. Acc: 72.98%\n",
      "Epoch: 17 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.743 | Train Acc: 74.03%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 73.69%\n",
      "Epoch: 18 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.717 | Train Acc: 74.95%\n",
      "\t Val. Loss: 0.750 |  Val. Acc: 75.07%\n",
      "Epoch: 19 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.708 | Train Acc: 75.24%\n",
      "\t Val. Loss: 0.736 |  Val. Acc: 74.99%\n",
      "Epoch: 20 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.695 | Train Acc: 75.85%\n",
      "\t Val. Loss: 0.804 |  Val. Acc: 73.27%\n",
      "Epoch: 21 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.658 | Train Acc: 77.19%\n",
      "\t Val. Loss: 0.716 |  Val. Acc: 76.89%\n",
      "Epoch: 22 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.645 | Train Acc: 77.48%\n",
      "\t Val. Loss: 0.713 |  Val. Acc: 76.25%\n",
      "Epoch: 23 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.622 | Train Acc: 78.34%\n",
      "\t Val. Loss: 0.709 |  Val. Acc: 76.80%\n",
      "Epoch: 24 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.615 | Train Acc: 78.35%\n",
      "\t Val. Loss: 0.702 |  Val. Acc: 76.95%\n",
      "Epoch: 25 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.600 | Train Acc: 78.98%\n",
      "\t Val. Loss: 0.741 |  Val. Acc: 75.99%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device, epoch)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#     writer.add_scalar('Epoch/Train/loss', train_loss, epoch)\n",
    "#     writer.add_scalar('Epoch/Train/accuracy', train_acc, epoch)\n",
    "#     writer.add_scalar('Epoch/Valid/loss', valid_loss, epoch)\n",
    "#     writer.add_scalar('Epoch/Valid/accuracy', valid_acc, epoch)\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7h-D2bEXVph"
   },
   "source": [
    "We then load the parameters of our model that achieved the best validation loss and evaluate this model on the test set to achieve a ~75% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "S4Z3pxPwIagX",
    "outputId": "232cdb45-68a7-4329-87e5-50fa7a653eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.700 | Test Acc: 76.51%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut3-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "3 - AlexNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
